# -*- coding: utf-8 -*-
"""Code_RajendraPrasad_Govindraju.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AOt1B8g5lLYb-340C5DVtLXD2Kkh_cbT
"""

# Libraries data handling
import numpy as np
import pandas as pd

# Librarires for visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(font = 'Serif', style = 'white', rc = {'axes.facecolor':'#f1f1f1', 'figure.facecolor':'#f1f1f1'})

# Reading the data
df = pd.read_csv('data.csv')

# Droping the first column, which is index
df.drop(columns='Unnamed: 0', inplace=True)

# Displaying the data
df.head()

"""### Dataset Description
The various features of the cleaned dataset are explained below:
1) Airline: The name of the airline company is stored in the airline column. It is a categorical feature having 6 different airlines.
2) Flight: Flight stores information regarding the plane's flight code. It is a categorical feature.
3) Source City: City from which the flight takes off. It is a categorical feature having 6 unique cities.
4) Departure Time: This is a derived categorical feature obtained created by grouping time periods into bins. It stores information about the departure time and have 6 unique time labels.
5) Stops: A categorical feature with 3 distinct values that stores the number of stops between the source and destination cities.
6) Arrival Time: This is a derived categorical feature created by grouping time intervals into bins. It has six distinct time labels and keeps information about the arrival time.
7) Destination City: City where the flight will land. It is a categorical feature having 6 unique cities.
8) Class: A categorical feature that contains information on seat class; it has two distinct values: Business and Economy.
9) Duration: A continuous feature that displays the overall amount of time it takes to travel between cities in hours.
10)Days Left: This is a derived characteristic that is calculated by subtracting the trip date by the booking date.
11) Price: Target variable stores information of the ticket price.
"""

# Shape of the data
df.shape

"""There are 10 features and one target variable and 300K rows"""

# Dataset info
df.info()

"""The data type is correct for all the variables."""

# Checking for missing values
df.isnull().sum()

"""There are no missing values."""

# Checking the mean, median, max 
df.describe().T

"""# ExploratoryDataAnalysis"""

numeric_var = ['duration', 'days_left']

fig, ax = plt.subplots(1,2, figsize = (10,5))
for axis, num_var in zip(ax, numeric_var):
    sns.boxplot(y = num_var,data = df, ax = axis, color = 'skyblue')
    axis.set_xlabel(f"{num_var}", fontsize = 12)
    axis.set_ylabel(None)

fig.suptitle('Box Plot for Identifying Outliers', fontsize = 20)
fig.text(0.5, -0.05, 'Numeric Features', ha = 'center', fontsize = 14)
plt.tight_layout()

"""`duration` contains some values which fall beyond the IQR (Inter Quantile Range), but we have to decide whether to call them outliers or not. For this case, I am considering 0.05 threshold for the outlier identification. Any data point which lies beyond 95 percentile is an outlier.

There can be instances where the `duration` is very high, but if we consider that, then model may not perform well.
"""

# Considering 95% percentile for duration
df = df[df['duration'] <= df['duration'].quantile(0.95)]

##price variation with Economy and Business class

fig, ax = plt.subplots(1,1, figsize = (10,5))
sns.kdeplot(x='price', data=df, hue='class')
ax.set_xlabel('Price', fontsize=12)
fig.suptitle('Distribution of price based on class', fontsize = 20);

#to check if price varies with Airline or not

plt.figure(figsize=(20, 10))
ax = sns.boxplot(x='airline', y='price', data=df)
ax.set_ylabel('Price', fontsize=16)
ax.set_xlabel('Airplines', fontsize=16)
ax.set_xticklabels(df['airline'].unique(), fontsize=14)
ax.set_title('Prices variation based on company', fontsize=22);

#impact on ticket price when its booked 1-2 days before depature?

plt.figure(figsize=(20, 10))
ax = sns.scatterplot(x='days_left', y='price', data=df.groupby(['days_left'])['price'].mean().reset_index())
ax.set_ylabel('Price (mean)', fontsize=16)
ax.set_xlabel('Days before departure', fontsize=16)
ax.set_title('Prices variation based on days left', fontsize=22);

"""# Does the price change with the duration of the flight? """

plt.figure(figsize=(20, 10))
ax = sns.regplot(x='duration', y='price',
                 data=df.groupby(['duration'])['price'].mean().reset_index(),
                 order=3)
ax.set_ylabel('Price (mean)', fontsize=16)
ax.set_xlabel('Duration of flight (hrs)', fontsize=16)
ax.set_title('Prices variation based on duration of flight', fontsize=22);

"""The price of the flight increases as the duration increase upto 20 hrs and after that it follows a downward trend. I think as the duration of flight is more, less people are interested in buying it, so the price drops.

<h3>  Does ticket price change based on the departure time and arrival time? </h3>
"""

df_time = df.melt(id_vars=['price'], value_vars=['arrival_time', 'departure_time'])
df_time.columns = ['price', 'time', 'time_1']

order = ['Early_Morning', 'Morning', 'Afternoon', 'Evening', 'Night', 'Late_Night']
plt.figure(figsize=(20, 10))
ax = sns.barplot(x='time_1', y='price', estimator=np.mean, data=df_time, hue='time', order=order)
ax.set_ylabel('Price (mean)', fontsize=16)
ax.set_xlabel('Time of flight', fontsize=16)
ax.set_xticklabels(order, fontsize=15)
ax.set_title('Prices variation based on time of flight', fontsize=22);

"""# How the price changes with change in Source and Destination?"""

plt.figure(figsize=(20, 10))
ax = sns.relplot(x='destination_city', y="price", data=df,  col="source_city", col_wrap=3, kind="line")
ax.fig.subplots_adjust(top=0.9)
ax.fig.suptitle('Airline prices based on the source and destination cities', fontsize=20);

"""The flight from Chennai to Banglore are costier than others, while boarding from Delhi is mostly cheaper. Also, boarding from Banglore is costlier than others.

<h2>  Machine Learning </h2>
"""

# Data prepration for ML
from sklearn.model_selection import train_test_split

# Train and test
# As there are many unique values of the flight, I am removing it
X = df.drop(columns=['price', 'flight'])
y = df['price']

# one hot encoding
X_encoded = pd.get_dummies(X, drop_first=True)

X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, train_size=0.8, random_state=99)

# Scaling the data
from sklearn.preprocessing import StandardScaler

scalar = StandardScaler()
X_train_scaled = pd.DataFrame(scalar.fit_transform(X_train), columns=X_train.columns)
X_test_scaled = pd.DataFrame(scalar.transform(X_test), columns=X_train.columns)

# Shape of the training data
X_train.shape

"""<h3> Linear Regression   </h3>"""

from sklearn.linear_model import LinearRegression

lr = LinearRegression()

def model_evaluation(model, name=''):
    model.fit(X_train_scaled, y_train)
    # Performance Evaluation
    print(f'The r2 score for {name} model on train set is : ', model.score(X_train_scaled, y_train))
    print(f'The r2 score for {name} model on test set is : ', model.score(X_test_scaled, y_test))
    
    # Coefficients
    df_lr = pd.DataFrame()
    df_lr['features'] = X_train_scaled.columns
    df_lr['coef'] = model.coef_

    # Selecting top 5 features
    print('\n Top 5 imp features : \n')
    print(df_lr.reindex(df_lr['coef'].abs().sort_values(ascending=False).index)[:5])

# Plain Linear Regression Model
model_evaluation(lr, 'Linear Regression')

"""The r2 score for both the train and test set are quite close, so there is no overfitting. Also, the r2 scores are around 91%, which good. This means that 91% variance in the flight price can be attributed to these features.

The economy class, affect the price of the flight most, which quite obious if the class is business then price is high else low. So the coefficient of the `class_Economy` is negetive.
"""

# L1 regularisation
from sklearn import linear_model
lasso = linear_model.Lasso()
model_evaluation(lasso, 'L1 Regularisation')

"""Very similar results as simple linear regression model"""

# L2 regularisation
ridge = linear_model.Ridge()
model_evaluation(ridge, 'L2 Regularisation')

"""The values of the coefficients of L2 regularisation and the performance are also very similar to the simple linear regresssion"""

# Elastic Net
elastic_net = linear_model.ElasticNet()
model_evaluation(elastic_net, 'Elastic Net')

"""The performance of the Elastic Net model is lower than the others also in the coefficients, `airline_Vistara` is more imp than `stops_zero`, which opposite for other models.

<h3> Support Vector Regression  </h3>
"""

from sklearn.svm import LinearSVR

# L1 Regularisation
svr_l1 = LinearSVR(fit_intercept='epsilon_insensitive')
model_evaluation(svr_l1, 'Support Vector L1 Regression')

"""The SVR L1 Regularisation is showing small drop in the performance compared to Linear Regression. The top coefficient remains the same, but there are some changes for the remaining positions."""

# L2 Regularisation
svr_l2 = LinearSVR(fit_intercept='squared_epsilon_insensitive')
model_evaluation(svr_l2, 'Support Vector L2 Regression')

"""The SVR L2 Regularisation is giving same performance that of L1 regularisation. Also, the top 5 imp features of these two are also same, though there is very small change in the coefficient values.

<h3> Random Forest Regression   </h3>
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

forest = RandomForestRegressor(n_estimators=200, oob_score=True)
parameters = {'max_depth': [3, 5, 7, 9],
             'min_samples_leaf' : [100, 500, 1000, 2000],
             'min_samples_split' : [100, 500, 1000, 2000]}

clf = GridSearchCV(forest, param_grid=parameters, cv=5)
clf.fit(X_train_scaled, y_train)

print(f'The r2 score for Random Forest model on train set is : ', clf.score(X_train_scaled, y_train))
print(f'The r2 score for Random Forest model on test set is : ', clf.score(X_test_scaled, y_test))

"""The performance of Random Forest model is better than all previous models. The r2 score is 99% which is very good. """

# Coefficients
df_forest = pd.DataFrame()
df_forest['features'] = X_train_scaled.columns
df_forest['coef'] = clf.feature_importances_

# Selecting top 5 features
print('\n Top 5 imp features : \n')
print(df_forest.reindex(df_forest['coef'].abs().sort_values(ascending=False).index)[:5])